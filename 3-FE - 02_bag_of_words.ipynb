{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(document):\n",
    "    return word_tokenize(document)\n",
    "\n",
    "def is_no_punctuation(word):\n",
    "    return word.isalnum()\n",
    "\n",
    "def is_no_stopword(word):\n",
    "    return word.lower() not in set(stopwords.words('english'))\n",
    "\n",
    "def stem(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def categorize(words):\n",
    "    tags = nltk.pos_tag(words)\n",
    "    return [tag for word, tag in tags]\n",
    "\n",
    "def lemmatize(words, tags):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    pos = [tag_dict.get(t[0].upper(), wordnet.NOUN) for t in tags]\n",
    "    return [lemmatizer.lemmatize(w, pos=p) for w, p in zip(words, pos)]\n",
    "\n",
    "def count(words):\n",
    "    return Counter(words)\n",
    "\n",
    "def sklearn_count_vect(data, **kwargs):\n",
    "    count_vect = CountVectorizer(**kwargs)\n",
    "    X_train_counts = count_vect.fit_transform(data)\n",
    "    print(X_train_counts)\n",
    "    print(count_vect.vocabulary_)\n",
    "    return X_train_counts\n",
    "\n",
    "def sklearn_tfidf_vect(data, **kwargs):\n",
    "    vec = TfidfVectorizer(**kwargs)\n",
    "    X_train_counts = vec.fit_transform(data)\n",
    "    print(X_train_counts)\n",
    "    print(vec.get_feature_names())\n",
    "    return X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Almost before we knew it, we had left the ground. The unknown holds its grounds.\"\n",
    "print('Document:', document)\n",
    "\n",
    "tokens = tokenize(document)\n",
    "print('Tokenized:', tokens)\n",
    "\n",
    "words = [w for w in tokens if is_no_punctuation(w) and is_no_stopword(w)]\n",
    "print('Stripped stopwords and punctuation:', words)\n",
    "\n",
    "words = stem(words)\n",
    "print('Stemming:', words)\n",
    "\n",
    "tokens_pos = categorize(tokens)\n",
    "words_pos = [pos for w, pos in zip(tokens, tokens_pos) if is_no_punctuation(w) and is_no_stopword(w)]\n",
    "print('Word categories:', words_pos)\n",
    "\n",
    "words = lemmatize(words, words_pos)\n",
    "print('Lemmatization:', words)\n",
    "\n",
    "bag_of_words = count(words)\n",
    "print('Bag of words:', bag_of_words)\n",
    "\n",
    "sklearn_count_vect([document])\n",
    "sklearn_count_vect([\" \".join(words)])\n",
    "sklearn_tfidf_vect([\" \".join(words)])\n",
    "#sklearn_count_vect([\" \".join(words)], ngram_range=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c39ce73b6d48f343ffd00681afb9b3f63104480cfaffe0ebb445fe41a6801158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('packt-repo-M2qY5kM-': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
