{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset into Panda Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer - https://learn.microsoft.com/en-us/azure/machine-learning/v1/how-to-create-register-datasets\n",
    "\n",
    "# retrieve an existing datastore in the workspace by name\n",
    "datastore_name = 'mldemoblob'\n",
    "datastore = Datastore.get(ws, datastore_name)\n",
    "print(datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TabularDataset from the file path in datastore\n",
    "datastore_path = [(datastore, 'melb_data.csv')]\n",
    "tabdf = Dataset.Tabular.from_delimited_files(path=datastore_path)\n",
    "print(tabdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase display of all columns of rows for panda datasets\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# create panda dataframe\n",
    "raw_df = tabdf.to_pandas_dataframe()\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Definition\n",
    "- Rooms: Number of rooms\n",
    "- Price: Price in dollars\n",
    "- Method:<br> S - property sold; <br>\n",
    "        SP - property sold prior;<br> \n",
    "        PI - property passed in; <br>\n",
    "        PN - sold prior not disclosed;<br> \n",
    "        SN - sold not disclosed; <br>\n",
    "        NB - no bid; <br>\n",
    "        VB - vendor bid; <br>\n",
    "        W - withdrawn prior to auction;<br> \n",
    "        SA - sold after auction; <br>\n",
    "        SS - sold after auction price not disclosed.<br> \n",
    "        N/A - price or highest bid not available.\n",
    "\n",
    "- Type:<br>   br - bedroom(s);<br> \n",
    "        h - house,cottage,villa, semi,terrace; <br>\n",
    "        u - unit, duplex; <br>\n",
    "        t - townhouse; <br>\n",
    "        dev site - development site; <br>\n",
    "        o res - other residential.\n",
    "        \n",
    "- SellerG: Real Estate Agent\n",
    "- Date: Date sold\n",
    "- Distance: Distance from CBD\n",
    "- Regionname: General Region (West, North West, North, North east â€¦etc)\n",
    "- Propertycount: Number of properties that exist in the suburb.\n",
    "- Bedroom2 : Scraped # of Bedrooms (from different source)\n",
    "- Bathroom: Number of Bathrooms\n",
    "- Car: Number of carspots\n",
    "- Landsize: Land Size\n",
    "- BuildingArea: Building Size\n",
    "- CouncilArea: Governing council for the area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape shows us the number of columns (features/pot. labels)= 21 and the number of rows (samples) = 13580\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34857 entries, 0 to 34856\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Suburb         34857 non-null  object        \n",
      " 1   Address        34857 non-null  object        \n",
      " 2   Rooms          34857 non-null  int64         \n",
      " 3   Type           34857 non-null  object        \n",
      " 4   Price          27247 non-null  float64       \n",
      " 5   Method         34857 non-null  object        \n",
      " 6   SellerG        34857 non-null  object        \n",
      " 7   Date           34857 non-null  datetime64[ns]\n",
      " 8   Distance       34856 non-null  float64       \n",
      " 9   Postcode       34856 non-null  float64       \n",
      " 10  Bedroom2       26640 non-null  float64       \n",
      " 11  Bathroom       26631 non-null  float64       \n",
      " 12  Car            26129 non-null  float64       \n",
      " 13  Landsize       23047 non-null  float64       \n",
      " 14  BuildingArea   13558 non-null  float64       \n",
      " 15  YearBuilt      15551 non-null  float64       \n",
      " 16  CouncilArea    34857 non-null  object        \n",
      " 17  Lattitude      26881 non-null  float64       \n",
      " 18  Longtitude     26881 non-null  float64       \n",
      " 19  Regionname     34857 non-null  object        \n",
      " 20  Propertycount  34854 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(1), object(7)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Suburb               0\n",
       "Address              0\n",
       "Rooms                0\n",
       "Type                 0\n",
       "Price             7610\n",
       "Method               0\n",
       "SellerG              0\n",
       "Date                 0\n",
       "Distance             1\n",
       "Postcode             1\n",
       "Bedroom2          8217\n",
       "Bathroom          8226\n",
       "Car               8728\n",
       "Landsize         11810\n",
       "BuildingArea     21299\n",
       "YearBuilt        19306\n",
       "CouncilArea          0\n",
       "Lattitude         7976\n",
       "Longtitude        7976\n",
       "Regionname           0\n",
       "Propertycount        3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Unique Values, Missing Values Percentage, Percentage of Values in the biggest category and Datatype\n",
    "stats = []\n",
    "for cl in raw_df.columns:\n",
    "    stats.append((cl, \n",
    "                  raw_df[cl].nunique(), \n",
    "                  raw_df[cl].isnull().sum(),\n",
    "                  raw_df[cl].isnull().sum() * 100 / raw_df.shape[0],\n",
    "                  raw_df[cl].value_counts(normalize=True, dropna=False).values[0] * 100,\n",
    "                  raw_df[cl].dtype))\n",
    "\n",
    "# create new dataframe containing the above mentioned stats    \n",
    "stats_df = pd.DataFrame(stats, columns=['Feature', \n",
    "                                        'Unique Values',\n",
    "                                        'Missing Values',\n",
    "                                        'Missing Values [%]',\n",
    "                                        'Values in the biggest category [%]',\n",
    "                                        'Datatype'])\n",
    "\n",
    "stats_df.sort_values('Missing Values [%]', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at the above results. What do we see?\n",
    "\n",
    "- we seem to have 4 features with missing values (BuildingArea, YearBuilt, CouncilArea, Car)\n",
    "\n",
    "- looking at the datatypes, there seem to be a lot of float64, even though a lot of them are very small integer numbers, like YearBuilt, Car, Bathroom, Bedroom2, Postcode, Price. Float64 stores as the name suggest data in 64-bit. On top of that, but probably not a problem in this case, it can only represent a real number to a certain extent of precision. Either way, most of these are small natural numbers, which we could store in int32 to save space.\n",
    "\n",
    "- there are 7 features of type Object, which means they are probably strings. We should have a look at them.<br>\n",
    "        Type has 3 distinct values, our definitions shows 6<br>\n",
    "        Method has 5 distinct values, our definition shows 11<br>\n",
    "        SellerG has 268 distinct seller names<br>\n",
    "        Address has 13378 distinct values, but we have 13580 samples, so there seems to be places with the same address<br>\n",
    "        Regionname has 8 distinct values, the regions of Melbourne<br>\n",
    "        Subburb has 314 distinct values, the suburbs of Melbourne<br>\n",
    "        CouncilArea has 33 distinct values, and is the only categorical feature with missing values<br>\n",
    "\n",
    "- we see there is a column called Price, which might be a good label/target for supervised training.\n",
    "\n",
    "Before doing anything else, lets clean some names and get rid of some features that might be not of too much interest for our first analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the Address and the Seller for now (we can add them later back into the mix)\n",
    "df = raw_df.drop(['Address', 'SellerG'],axis=1)\n",
    "# rename some of the columns\n",
    "df = df.rename(columns={'Bedroom2': 'Bedrooms', 'Bathroom': 'Bathrooms','Regionname': 'Region', 'Car': 'Parking', 'Propertycount': 'SuburbPropCount'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicated entries\n",
    "s = df.duplicated(keep = False)\n",
    "s = s[s == True]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[7769,7770]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets drop one of them in the dataframe\n",
    "df.drop([7769], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abbreviated Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the features with missing categories compared to the definition\n",
    "df['Method'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:<br>\n",
    "        S - property sold; <br>\n",
    "        SP - property sold prior; <br>\n",
    "        PI - property passed in; <br>\n",
    "        VB - vendor bid; <br>\n",
    "        SA - sold after auction; \n",
    "\n",
    "We are missing:<br>\n",
    "        W - withdrawn prior to auction; <br>\n",
    "        SS - sold after auction price not disclosed. <br>\n",
    "        N/A - price or highest bid not available.<br>\n",
    "        PN - sold prior not disclosed; <br>\n",
    "        SN - sold not disclosed; <br>\n",
    "        NB - no bid; \n",
    "\n",
    "Apparently, there has been already some cleaning being done on this dataset and the entries of unsold houses or houses without a price have been discarded.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the features with missing categories compared to the definition\n",
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:<br>\n",
    "           h - house,cottage,villa,semi,terrace; <br>\n",
    "           u - unit, duplex; <br>\n",
    "           t - townhouse; \n",
    "           \n",
    "We do not have:       \n",
    "           br - bedroom(s); <br>\n",
    "           dev site - development site; <br>\n",
    "           o res - other residential.\n",
    "\n",
    "Apparently also here, the data was precleaned, removing single bedroom offers, developement sites and others, leaving us with houses, units and townhouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace abbreviations\n",
    "df = df.replace({'Type': {'h':'house','u':'unit','t':'townhouse'}})\n",
    "df = df.replace({'Method': {'S':'Property Sold','SP':'Property Sold Prior','PI':'Property Passed In',\n",
    "                            'VB':'Vendor Bid', 'SA':'Sold After Auction'}})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postcodes vs Suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One might think, what about the postcode and the suburb, how are they connected. Let's have a look:\n",
    "postcodes_df = df.groupby('Postcode', as_index=False).Suburb.nunique()\n",
    "postcodes_df.columns = ['Postcode', '#Assigned Suburbs']\n",
    "postcodes_df.loc[postcodes_df['#Assigned Suburbs'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_df.loc[postcodes_df['#Assigned Suburbs'] > 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of 198 postcodes, 73 are used for multiple suburbs. still, the postcodes are a subset of the suburbs and probably not necessary. Judging by this, let us remove the postcode for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Postcode'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CouncilArea'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is a category called \"Unavailable\" and the missing values labelled with None. We will come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Suburb'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, at this point we should use fuzzy matching techniques to see, if there are duplicate entries that are written similarly or have a typo or a space somewhere, but we leave it at this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show statistical properties for the numerical features, the lambda converts values into a more readable format (float is shown in scientific notion)\n",
    "dist_df = df.describe().T.apply(lambda s: s.apply(lambda x: format(x, 'g')))\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some other information missing to the statistics\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "max_count=[]\n",
    "min_count=[]\n",
    "mode_count=[]\n",
    "mode=[]\n",
    "skew=[]\n",
    "for cl in df.columns:\n",
    "    if (is_numeric_dtype(df[cl])):\n",
    "        max_count.append(df[cl].value_counts(dropna=False).loc[df[cl].max()])\n",
    "        min_count.append(df[cl].value_counts(dropna=False).loc[df[cl].min()])\n",
    "        mode_count.append(df[cl].value_counts(dropna=False).loc[df[cl].mode()[0]])\n",
    "        skew.append(df[cl].skew())\n",
    "        mode.append(int(df[cl].mode()[0]))\n",
    "\n",
    "dist_df['mode'] = mode\n",
    "dist_df['skew'] = skew\n",
    "dist_df['#values(min)'] = min_count\n",
    "dist_df['#values(max)'] = max_count\n",
    "dist_df['#values(mode)'] = mode_count\n",
    "dist_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points of interest for further analysis:\n",
    "\n",
    "- Price: Skewed to the right, we will probably see some few high prices. Not surprising.\n",
    "- Distance: Skewed to the right, probably due to the one sample being 48.1km away from the CBD in Melbourne. Interestingly enough there are 6 samples with 0 distance. Sometimes 0 is a dummy value, so we should check that out. Judging by the fact that the mode 11 is set 739 times, the distance might not be exactly the distance from the city center, but the mean distance of a suburb perhaps from the city center. We should check this out.\n",
    "- Bedrooms: Skewed to the right, due to some high amounts of bedrooms in some places. Curiously there are 16 samples with 0 bedrooms, which needs to be checked.\n",
    "- Bathrooms: same as for bedrooms, with 34 samples of 0 bathrooms, which sounds weird.\n",
    "- Parking: same as for bedrooms, but here 1026 samples with no parking spaces is not surprising.\n",
    "- Landsize: Extremely skewed (95.24) to the right. The max is 433014. If we presume m2, these are about 43 hectar of land. Not impossible, but it probably would distort our modelling. \n",
    "- BuildingArea: Also extremely skewed to the right, due to the maximum size of 44515 m2. This sounds improbable, so we might want to remove that one. Also there are 17 samples with 0 building area, which also does not sound good.\n",
    "- YearBuilt: skewed to the left due to the one building built in 1196.\n",
    "- SuburbPropCount: slightly skewed the right. We have to see how helpful this value is.\n",
    "\n",
    "Let's go through these points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x=\"Price\",points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we presumed, still a bunch of prices past the upper fence (2.35M). Lets create a new field as the log of Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Price_log\"] = np.log(df['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x=\"Price_log\",points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks far more natural. Let's follow this one throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Distance'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Bedrooms'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BuildingArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, y=\"BuildingArea\",points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very distorted box plot. Hovering over the box you can see the statistics. The upper fence is at 295, which means statistically (presuming a normal distribution) everything above that is an outlier. Lets have a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[raw_df['BuildingArea'] > 295]['BuildingArea'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a lot of samples in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking area over 2000 leaves us with 4 examples\n",
    "df.loc[raw_df['BuildingArea'] > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the last house is 48.1 km or miles away from the city center, therefore having a landsize and building area in those values are feasible. Though if we want to understand house prices in Melbourne, this might not be that important. It is also in the Northern Victoria region and not in the metropolitan regions. We could go further here to have a look in the connection between these specific houses outside of the norm in conjunction with other features, but we will leave it at this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([13245], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[raw_df['BuildingArea'] > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking Landsize\n",
    "fig = px.box(df, y=\"Landsize\",points=\"all\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same seems to be true for Landsize. The upper fence is at 1357, but we even see one outlier over 400000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[raw_df['Landsize'] > 1357]['Landsize'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a lot of houses in this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let check for a higher number to get some samples\n",
    "df.loc[raw_df['Landsize'] > 30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let us store our first cleansing as a dataset in Azure ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.Tabular.register_pandas_dataframe(dataframe = df, target = datastore, name ='Melbourne Housing Dataset', description = 'Data Cleansing 1 - removed address, postcode, duplicates and outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we would drop any row with an empty value, we would loose half the dataset (6196 rows left). Lets look at the row distribution of missing values.\n",
    "df.dropna(how='any').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values for CouncilArea seem to be at the end of the list, the one for parking also very localized, the others ones are all over the place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CouncilArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CouncilArea'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.CouncilArea.isin(['Unavailable'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CouncilArea'].fillna(value = \"Missing\", inplace = True)\n",
    "df['CouncilArea'].replace(to_replace=\"Unavailable\", value=\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CouncilArea'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way would be to find a list of CouncilAreas and fill the correct values in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BuildingArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest way, replace the building area by the mean value of the other entries\n",
    "BA_mean = df['BuildingArea'].mean()\n",
    "df['BuildingArea'].replace(to_replace=np.nan, value=BA_mean, inplace=True)\n",
    "df['BuildingArea'].isnull().sum()\n",
    "BA_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YearBuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YB_median = df['YearBuilt'].median()\n",
    "df['YearBuilt'].replace(to_replace=np.nan, value=YB_median, inplace=True)\n",
    "df['YearBuilt'].isnull().sum()\n",
    "YB_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PK_median = df['Parking'].median()\n",
    "df['Parking'].replace(to_replace=np.nan, value=PK_median, inplace=True)\n",
    "df['Parking'].isnull().sum()\n",
    "PK_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.Tabular.register_pandas_dataframe(dataframe = df, target = datastore, name ='Melbourne Housing Dataset', description = 'Data Cleansing 2 - replaced missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the correlation between different features with a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "# set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "# generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are missing features. There are 13 features shown, which are all the numerical columns. This means, the algorithm cannot handle our objects and datetime columns. Let us change that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Categorical Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert all columns in the object dataframe to the \"categorical\" datatype\n",
    "for cl in obj_df.columns:\n",
    "    obj_df[cl] = obj_df[cl].astype('category')\n",
    "    \n",
    "obj_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in obj_df.columns:\n",
    "    obj_df[cl+\"_cat\"] = obj_df[cl].cat.codes\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will incorporate the numbered versions of our categorical data into a new dataframe\n",
    "column_replacement = {'Type':'Type_cat','Suburb':'Suburb_cat','Method':'Method_cat','CouncilArea':'CouncilArea_cat','Region':'Region_cat'}\n",
    "cont_df = df.copy()\n",
    "for key in column_replacement:\n",
    "    cont_df[key] = obj_df[column_replacement[key]]\n",
    "cont_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df['Date_Epoch'] = cont_df['Date'].apply(lambda x: x.timestamp())\n",
    "cont_df.drop(['Date'], axis=1, inplace=True)\n",
    "cont_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in cont_df.columns:\n",
    "    if (cont_df[cl].dtype == np.float64 and cl not in ['Lattitude', 'Longtitude', 'Price_log', 'Distance']):\n",
    "        cont_df[cl] = cont_df[cl].astype('int')\n",
    "cont_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with converted categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us do the correlation again\n",
    "# Compute the correlation matrix\n",
    "corr = cont_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we see:\n",
    "\n",
    "- Rooms is strongly correlated with Price, Price_log, Distance, Bedrooms, Bathrooms, Parking and Building Area\n",
    "- Type is strongly correlated with Price, Price_log, Bedrooms, YearBuilt and Rooms\n",
    "- Price is strongly correlated with Rooms, Type, Bedrooms, Bathrooms, Parking BuildingArea\n",
    "- Suburb, Method, Landsize and SuburbPropCount seem not to have to much influence in its current state on other features or the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.Tabular.register_pandas_dataframe(dataframe = cont_df, target = datastore, name ='Melbourne Housing Dataset', description = 'Data Cleansing 3 - all features converted to numerical values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price as the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# create X and Y vectors\n",
    "ignored_col = ['Price', 'Price_log', 'Date']\n",
    "cols = [c for c in cont_df.columns if c not in ignored_col]\n",
    "X = cont_df[cols]\n",
    "y = cont_df['Price']\n",
    "\n",
    "# define the regression forest\n",
    "forest = ExtraTreesRegressor(n_estimators=250, criterion='mse', random_state=0, max_depth=10)\n",
    "\n",
    "# run the forest fitting with our vectors\n",
    "forest.fit(X, y)\n",
    "\n",
    "# create indices order for feature importance\n",
    "importances = forest.feature_importances_\n",
    "tree_importances = np.array([tree.feature_importances_ for tree in forest.estimators_])\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "imp_df = pd.DataFrame([tree.feature_importances_ for tree in forest.estimators_], columns = cols)\n",
    "\n",
    "# sort columns by importance\n",
    "imp_df = imp_df[[cols[i] for i in indices.tolist()]]\n",
    "imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating barplot to visualize feature importance\n",
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(n_colors=3)\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = sns.barplot(data=imp_df, color=palette[1], capsize=.2, errwidth=1.2)\n",
    "plt.title(\"Feature importances\")\n",
    "plt.xticks(range(X.shape[1]), rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log(Price) Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# create X and Y vectors\n",
    "ignored_col = ['Price', 'Price_log', 'Date']\n",
    "cols = [c for c in cont_df.columns if c not in ignored_col]\n",
    "X = cont_df[cols]\n",
    "y = cont_df['Price_log']\n",
    "\n",
    "# define the regression forest\n",
    "forest = ExtraTreesRegressor(n_estimators=250, criterion='mse', random_state=0, max_depth=10)\n",
    "\n",
    "# run the forest fitting with our vectors\n",
    "forest.fit(X, y)\n",
    "\n",
    "# create indices order for feature importance\n",
    "importances = forest.feature_importances_\n",
    "tree_importances = np.array([tree.feature_importances_ for tree in forest.estimators_])\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "imp_df = pd.DataFrame([tree.feature_importances_ for tree in forest.estimators_], columns = cols)\n",
    "\n",
    "# sort columns by importance\n",
    "imp_df = imp_df[[cols[i] for i in indices.tolist()]]\n",
    "imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "palette = sns.color_palette(n_colors=3)\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = sns.barplot(data=imp_df, color=palette[1], capsize=.2, errwidth=1.2)\n",
    "plt.title(\"Feature importances\")\n",
    "plt.xticks(range(X.shape[1]), rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging deeper on correlation between Price and Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, y=\"Price_log\",x='Type', color = 'Type', \n",
    "                 category_orders={\"Type\": [\"house\", \"townhouse\", \"unit\"]})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d51804c197868cb8ed22899dcd318e00f8da179c4203b30505e711f2c519d65a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
